{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da26ed47-60d4-433c-ac2f-8f37044603ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# install dependencies\n",
    "%pip install -e ..\n",
    "%pip install git+https://github.com/end-to-end-mlops-databricks-3/marvelous@0.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b43bd5dc-5eeb-43a9-ad62-a1ee5fc05226",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#restart python\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "863aa899-a57a-438e-a402-725db941ace0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# system path update, must be after %restart_python\n",
    "# caution! This is not a great approach\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "832be333-6fe4-42e8-ab91-0b356d2a92df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# A better approach (this file must be present in a notebook folder, achieved via synchronization) %pip install insurance-1.0.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "851a84f6-b1a6-4565-9402-026ba0f4fa01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import mlflow\n",
    "\n",
    "from insurance.config import ProjectConfig\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from lightgbm import LGBMRegressor\n",
    "from mlflow.models import infer_signature\n",
    "from marvelous.common import is_databricks\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from mlflow import MlflowClient\n",
    "import pandas as pd\n",
    "from insurance import __version__\n",
    "from mlflow.utils.environment import _mlflow_conda_env\n",
    "from databricks import feature_engineering\n",
    "from databricks.feature_engineering import FeatureFunction, FeatureLookup\n",
    "from pyspark.errors import AnalysisException\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2515559-a175-4f7c-b193-f0654a81cc75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config = ProjectConfig.from_yaml(config_path=\"../project_config.yml\", env=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cba51f96-90ce-4e2d-8095-bc71dc4352ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "fe = feature_engineering.FeatureEngineeringClient()\n",
    "\n",
    "train_set = spark.table(f\"{config.catalog_name}.{config.schema_name}.train_set\")\n",
    "test_set = spark.table(f\"{config.catalog_name}.{config.schema_name}.test_set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32bddf39-486a-452b-8ff6-ed9741e98041",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Option 1: feature engineering client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb99f828-4966-4c97-a396-cf10ec912d32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aab7a59b-5d98-49c4-944b-8eb948a6fbe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Add a surrogate 'Id' column to train_set\n",
    "\n",
    "train_set_with_id = train_set.withColumn(\"Id\", monotonically_increasing_id())\n",
    "\n",
    "feature_table_name = f\"{config.catalog_name}.{config.schema_name}.insurance_features_demo\"\n",
    "feature_table_name_sql = f\"{config.catalog_name}.{config.schema_name}.insurance_features_demo_sql\"\n",
    "lookup_features = [\"age\", \"bmi\", \"children\"] #features that we pretend we dont`t have it in the train_set and we lookup in the table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43654198-aebc-461c-99ff-3e05b7709b79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Define the feature table\n",
    "feature_table = fe.create_table(\n",
    "    name=feature_table_name,\n",
    "    primary_keys=[\"Id\"],\n",
    "    df=train_set_with_id.select([\"Id\"] + lookup_features),\n",
    "    description=\"Insurance features table\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "296ebc17-199e-4bb5-8c64-fb99bdd00c4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"ALTER TABLE {feature_table_name} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9c91b66-b99e-4348-b2c5-3729e70f6c1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT count(*) FROM mlops_dev.pirvugeo.insurance_features_demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8709a869-6a72-45da-a208-5da383a0a590",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fe.write_table(\n",
    "   name=feature_table_name,\n",
    "   df=train_set_with_id[[\"Id\"]+lookup_features],\n",
    "   mode=\"merge\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3c2e71c-6bcc-4039-b174-aa8f0663aee2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Option 2: SQL - create feature table with information about insurance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6afbc178-cdc0-4b61-996f-e828ad3e98a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "          CREATE OR REPLACE TABLE {feature_table_name_sql}\n",
    "          (Id BIGINT NOT NULL GENERATED ALWAYS AS IDENTITY (START WITH 1 INCREMENT BY 1), age BIGINT, bmi DOUBLE, children BIGINT);\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b74cfd8-fec1-495d-9889-c40ab74ebb74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# primary key on Databricks is not enforced!\n",
    "try:\n",
    "    spark.sql(f\"ALTER TABLE {feature_table_name_sql} ADD CONSTRAINT insurance_pk PRIMARY KEY(Id);\")\n",
    "except AnalysisException:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fa3dc07-3198-4dd5-b4cd-a9ac1c300b5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"ALTER TABLE {feature_table_name_sql} SET TBLPROPERTIES (delta.enableChangeDataFeed = true);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7378d462-908a-450c-a37a-0bfe84406d54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "          INSERT INTO {feature_table_name_sql} (age, bmi, children)\n",
    "          SELECT age, bmi, children\n",
    "          FROM {config.catalog_name}.{config.schema_name}.train_set\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a809c8b-d576-4480-aeb5-4a0184b4fe53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Create feature function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57228249-1fdb-4e1d-868b-3dabf4ef5e4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create feature function\n",
    "# docs: https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-syntax-ddl-create-sql-function\n",
    "\n",
    "# problems with feature functions:\n",
    "# functions are not versioned \n",
    "# functions may behave differently depending on the runtime (and version of packages and python)\n",
    "# there is no way to enforce python version & package versions for the function \n",
    "# this is only supported from runtime 17\n",
    "# advised to use only for simple calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32b496f9-7e97-4253-aa92-007e088bb222",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "function_name = f\"{config.catalog_name}.{config.schema_name}.average_age_demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a61b5e1-abd9-47b1-8098-f041aefe047c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fet = spark.sql( f\"\"\"\n",
    "select * from {feature_table_name_sql} \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd24686b-986e-4f27-a4ae-555bc9c78386",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Option 1: with Python\n",
    "spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE FUNCTION {function_name}(age long)\n",
    "        RETURNS string\n",
    "        LANGUAGE PYTHON AS\n",
    "        $$\n",
    "        if age < 18:\n",
    "            return \"minor\"\n",
    "        elif age < 65:\n",
    "            return \"adult\"\n",
    "        else:\n",
    "            return \"senior\"\n",
    "        $$\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc0d1841-2f4b-433b-8477-69c9c3169849",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# execute function\n",
    "maturity = spark.sql(f\"SELECT {function_name}(70) as age_maturity;\")\n",
    "maturity.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d0d09ea-fc40-4f63-b8f0-9536690e07df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Option 2\n",
    "spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE FUNCTION {function_name}_sql (age long)\n",
    "        RETURNS string\n",
    "        RETURN \n",
    "            CASE \n",
    "                WHEN age < 18 THEN 'minor'\n",
    "                WHEN age < 65 THEN 'adult'\n",
    "                ELSE 'senior'\n",
    "        End\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53e95d69-48ba-4499-8ec3-52549664f217",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "maturity = spark.sql(f\"SELECT {function_name}_sql(17) as age_maturity;\")\n",
    "maturity.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "414379ed-804a-44c7-b29e-d2d3fd90ee9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create a training set\n",
    "training_set = fe.create_training_set(\n",
    "    df=train_set_with_id.drop(\"age\", \"bmi\", \"children\"),\n",
    "    label=config.target,\n",
    "    feature_lookups=[\n",
    "        FeatureLookup(\n",
    "            table_name=feature_table_name,\n",
    "            feature_names=[\"age\", \"bmi\", \"children\"],\n",
    "            lookup_key=\"Id\",\n",
    "                ),\n",
    "        FeatureFunction(\n",
    "            udf_name=function_name,\n",
    "            output_name=\"maturity_age\",\n",
    "            input_bindings={\"age\": \"age\"},\n",
    "            ),\n",
    "    ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32c7561b-f025-48fb-97ef-cdcef2cad499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train & register a model\n",
    "training_df = training_set.load_df().toPandas()\n",
    "X_train = training_df[config.num_features + config.cat_features ]\n",
    "y_train = training_df[config.target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd790743-c73a-46cc-bb57-bcc05363df02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(X_train.dtypes)\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9791e7a7-4633-4205-8720-8b046bb19beb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"n_estimators\": 100,\n",
    "    \"num_leaves\": 31\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(\n",
    "        steps=[(\"preprocessor\", ColumnTransformer(\n",
    "            transformers=[(\"cat\", OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "                           config.cat_features)],\n",
    "            remainder=\"passthrough\")\n",
    "            ),\n",
    "               (\"regressor\", LGBMRegressor(**params))]\n",
    "        )\n",
    "\n",
    "print(config.parameters)\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94556afc-a0d1-4c90-9e33-85ccd0575628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"/Shared/demo-model-insurance-fe\")\n",
    "with mlflow.start_run(run_name=\"demo-model-insurance-fe\",\n",
    "                      tags={\"git_sha\": \"01234567890george\",\n",
    "                            \"branch\": \"feature3\"},\n",
    "                            description=\"demo run for FE model logging\") as run:\n",
    "    # Log parameters and metrics\n",
    "    run_id = run.info.run_id\n",
    "    mlflow.log_param(\"model_type\", \"LightGBM with preprocessing\")\n",
    "    mlflow.log_params(config.parameters)\n",
    "\n",
    "    # Log the model with feature engineering client. important\n",
    "    signature = infer_signature(model_input=X_train, model_output=pipeline.predict(X_train))\n",
    "    fe.log_model(\n",
    "                model=pipeline,\n",
    "                flavor=mlflow.sklearn,\n",
    "                artifact_path=\"lightgbm-pipeline-model-fe\",\n",
    "                training_set=training_set,\n",
    "                signature=signature,\n",
    "            )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0c42f74-57d0-4757-b8d3-7cfded4081a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_name = f\"{config.catalog_name}.{config.schema_name}.model_insurance_fe_demo\"\n",
    "model_version = mlflow.register_model(\n",
    "    model_uri=f'runs:/{run_id}/lightgbm-pipeline-model-fe',\n",
    "    name=model_name,\n",
    "    tags={\"git_sha\": \"01234567890george\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3305ec5-4976-47ec-b7dc-2dd9f4f0b67d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(test_set.dtypes)\n",
    "print(test_set.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42047889-718a-4b0c-95ba-62aca5fef73a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "test_set_with_id = test_set.withColumn(\"Id\", monotonically_increasing_id())\n",
    "test_set_with_id = test_set_with_id.withColumn(\"update_timestamp_utc\", current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2490ee36-2729-42b4-bb8f-ea3a612fcd56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# make predictions\n",
    "#fe.score_batch only works with pyspark, with pandas will fail\n",
    "features = [f for f in [\"Id\", \"update_timestamp_utc\"] + config.num_features + config.cat_features if f not in lookup_features]\n",
    "predictions = fe.score_batch(\n",
    "    model_uri=f\"models:/{model_name}/{model_version.version}\",\n",
    "    df=test_set_with_id[features]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cab87e91-c89b-4290-af00-a56e7d8d7326",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "client = MlflowClient()\n",
    "model_version = client.get_model_version(name=model_name, version=model_version.version)\n",
    "print(model_version.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cee8eda-f6fb-4e13-ba4c-cffe822468c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions.select(\"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45f77e2d-a38a-416c-be38-25bb309c3fea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "features = [f for f in [\"Id\", \"update_timestamp_utc\"] + config.num_features + config.cat_features if f not in lookup_features]\n",
    "test_set_with_new_id = test_set_with_id.select(*features).withColumn(\n",
    "    \"Id\",\n",
    "    (col(\"Id\").cast(\"bigint\") + 1000000))\n",
    "\n",
    "predictions = fe.score_batch(\n",
    "    model_uri=f\"models:/{model_name}/{model_version.version}\",\n",
    "    df=test_set_with_new_id[features]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45bc282a-316f-4ee2-b7e6-e72b46f348bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# make predictions for a non-existing entry -> error!\n",
    "predictions.select(\"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03486700-dfd7-411c-b91d-312dea05f14f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# replace none values or missing values with avg or other\n",
    "\n",
    "age_function = f\"{config.catalog_name}.{config.schema_name}.replace_age_missing\"\n",
    "spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE FUNCTION {age_function}(age bigint)\n",
    "        RETURNS bigint\n",
    "        LANGUAGE PYTHON AS\n",
    "        $$\n",
    "        if age is None:\n",
    "            return 35\n",
    "        else:\n",
    "            return age\n",
    "        $$\n",
    "        \"\"\")\n",
    "\n",
    "bmi_function = f\"{config.catalog_name}.{config.schema_name}.replace_bmi_missing\"\n",
    "spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE FUNCTION {bmi_function}(bmi DOUBLE)\n",
    "        RETURNS DOUBLE\n",
    "        LANGUAGE PYTHON AS\n",
    "        $$\n",
    "        if bmi is None:\n",
    "            return 30\n",
    "        else:\n",
    "            return bmi\n",
    "        $$\n",
    "        \"\"\")\n",
    "\n",
    "children_function = f\"{config.catalog_name}.{config.schema_name}.replace_children_missing\"\n",
    "spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE FUNCTION {children_function}(children bigint)\n",
    "        RETURNS bigint\n",
    "        LANGUAGE PYTHON AS\n",
    "        $$\n",
    "        if children is None:\n",
    "            return 2\n",
    "        else:\n",
    "            return children\n",
    "        $$\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63bf5749-0c6a-491c-bfdc-8128e0a13317",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# what if we want to replace with a default value if entry is not found\n",
    "# what if we want to look up value in another table? the logics get complex\n",
    "# problems that arize: functions/ lookups always get executed (if statememt is not possible)\n",
    "# it can get slow...\n",
    "\n",
    "# step 1: create 3 feature functions\n",
    "\n",
    "# step 2: redefine create training set\n",
    "\n",
    "# try again\n",
    "\n",
    "# create a training set\n",
    "training_set = fe.create_training_set(\n",
    "    df=train_set_with_id.drop(\"age\", \"bmi\", \"children\"),\n",
    "    label=config.target,\n",
    "    feature_lookups=[\n",
    "        FeatureLookup(\n",
    "            table_name=feature_table_name,\n",
    "            feature_names=[\"age\", \"bmi\", \"children\"],\n",
    "            lookup_key=\"Id\",\n",
    "            rename_outputs={\"age\": \"lookup_age\",\n",
    "                            \"bmi\": \"lookup_bmi\",\n",
    "                            \"children\": \"lookup_children\"}\n",
    "                ),\n",
    "        FeatureFunction(\n",
    "            udf_name=age_function,\n",
    "            output_name=\"age\",\n",
    "            input_bindings={\"age\": \"lookup_age\"},\n",
    "            ),\n",
    "        FeatureFunction(\n",
    "            udf_name=bmi_function,\n",
    "            output_name=\"bmi\",\n",
    "            input_bindings={\"bmi\": \"lookup_bmi\"},\n",
    "        ),\n",
    "        FeatureFunction(\n",
    "            udf_name=children_function,\n",
    "            output_name=\"children\",\n",
    "            input_bindings={\"children\": \"lookup_children\"},\n",
    "        ),\n",
    "        FeatureFunction(\n",
    "            udf_name=function_name,\n",
    "            output_name=\"maturity_age\",\n",
    "            input_bindings={\"age\": \"age\"},\n",
    "            ),\n",
    "    ],\n",
    "    exclude_columns=[\"update_timestamp_utc\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e1bc219-7063-4678-9be2-216b6043cde2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Train & register a model\n",
    "training_df = training_set.load_df().toPandas()\n",
    "X_train = training_df[config.num_features + config.cat_features ]\n",
    "y_train = training_df[config.target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8044777-7928-4f76-a883-db4c4aa91686",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"n_estimators\": 100,\n",
    "    \"num_leaves\": 31\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(\n",
    "        steps=[(\"preprocessor\", ColumnTransformer(\n",
    "            transformers=[(\"cat\", OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "                           config.cat_features)],\n",
    "            remainder=\"passthrough\")\n",
    "            ),\n",
    "               (\"regressor\", LGBMRegressor(**params))]\n",
    "        )\n",
    "\n",
    "print(config.parameters)\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fc129a3-f69c-4825-882b-c1e802da14e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"/Shared/demo-model-insurance-fe\")\n",
    "with mlflow.start_run(run_name=\"demo-model-insurance-fe\",\n",
    "                      tags={\"git_sha\": \"01234567890george\",\n",
    "                            \"branch\": \"feature3\"},\n",
    "                            description=\"demo run for FE model logging\") as run:\n",
    "    # Log parameters and metrics\n",
    "    run_id = run.info.run_id\n",
    "    mlflow.log_param(\"model_type\", \"LightGBM with preprocessing\")\n",
    "    mlflow.log_params(config.parameters)\n",
    "\n",
    "    # Log the model with feature engineering client. important\n",
    "    signature = infer_signature(model_input=X_train, model_output=pipeline.predict(X_train))\n",
    "    fe.log_model(\n",
    "                model=pipeline,\n",
    "                flavor=mlflow.sklearn,\n",
    "                artifact_path=\"lightgbm-pipeline-model-fe\",\n",
    "                training_set=training_set,\n",
    "                signature=signature,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9156f4d-60aa-4e74-a533-f7cde51ef57d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_name = f\"{config.catalog_name}.{config.schema_name}.model_insurance_fe_demo\"\n",
    "model_version = mlflow.register_model(\n",
    "    model_uri=f'runs:/{run_id}/lightgbm-pipeline-model-fe',\n",
    "    name=model_name,\n",
    "    tags={\"git_sha\": \"01234567890george\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a588698-da59-45d8-9172-3e8ffba9bc85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "features = [f for f in [\"Id\", \"update_timestamp_utc\"] + config.num_features + config.cat_features if f not in lookup_features]\n",
    "test_set_with_new_id = test_set_with_id.select(*features).withColumn(\n",
    "    \"Id\",\n",
    "    (col(\"Id\").cast(\"bigint\") + 1000000))\n",
    "\n",
    "predictions = fe.score_batch(\n",
    "    model_uri=f\"models:/{model_name}/{model_version.version}\",\n",
    "    df=test_set_with_new_id[features]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2eb9685-bcbe-4f36-93a9-0aa2fd6b2f29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_set_with_id.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25528d60-af4e-4e84-be42-d0aa2a23a976",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# make predictions for a non-existing entry -> no error!\n",
    "predictions.select(\"prediction\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ced059fa-26ed-444c-9d98-10bdb0001cc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Feature Engineering with DynamoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ac625d4-64f8-4cba-b5e1-0a19f8a051ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "region_name = \"eu-west-1\"\n",
    "aws_access_key_id = os.environ[\"aws_access_key_id\"]\n",
    "aws_secret_access_key = os.environ[\"aws_secret_access_key\"]\n",
    "\n",
    "client = boto3.client(\n",
    "    'dynamodb',\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    region_name=region_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c90f6b1-c94b-41dc-95b2-4f3986e351de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "response = client.create_table(\n",
    "    TableName='InsuranceFeatures',\n",
    "    KeySchema=[\n",
    "        {\n",
    "            'AttributeName': 'Id',\n",
    "            'KeyType': 'HASH'  # Partition key\n",
    "        }\n",
    "    ],\n",
    "    AttributeDefinitions=[\n",
    "        {\n",
    "            'AttributeName': 'Id',\n",
    "            'AttributeType': 'N'  # Number\n",
    "        }\n",
    "    ],\n",
    "    ProvisionedThroughput={\n",
    "        'ReadCapacityUnits': 5,\n",
    "        'WriteCapacityUnits': 5\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6cbc635-46c4-4804-a609-446a7d5fff19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Table creation initiated:\", response['TableDescription']['TableName'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a6c9323-c32d-48a4-b7ee-676c65c974b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client.put_item(\n",
    "    TableName='InsuranceFeatures',\n",
    "    Item={\n",
    "        'Id': {'N': '261898'},\n",
    "        'age': {'N': '35'},\n",
    "        'bmi': {'N': '30'},\n",
    "        'children': {'N': '2'}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99c6a24c-3500-455f-baaf-90dcf275d610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "response = client.get_item(\n",
    "    TableName='InsuranceFeatures',\n",
    "    Key={\n",
    "        'Id': {'N': '261898'}\n",
    "    }\n",
    ")\n",
    "# Extract the item from the response\n",
    "item = response.get('Item')\n",
    "print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4082d15-efc7-4e09-b4f3-ba19360b620b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "rows = spark.table(feature_table_name).toPandas().to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f64d78a-9f9d-47f6-a220-a0c267633ede",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def to_dynamodb_item(row):\n",
    "    return {\n",
    "        'PutRequest': {\n",
    "            'Item': {\n",
    "                'Id': {'N': str(row['Id'])}, \n",
    "                'bmi': {'N': str(row['bmi'])},\n",
    "                'children': {'N': str(row['children'])},\n",
    "                'age': {'N': str(row['age'])} # DynamoDB expects all values to be passed as strings,\n",
    "                \n",
    "                \n",
    "            }\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f32ee6d8-f12c-424f-9543-519375cb9b3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "items = [to_dynamodb_item(row) for row in rows]\n",
    "print(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0f3dfb6-db4a-4e7a-a409-5dedd4a3dc63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21c0f7d9-cbe2-4241-b9e7-a42302f51538",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for batch in chunks(items, 25):\n",
    "    response = client.batch_write_item(\n",
    "        RequestItems={\n",
    "            'InsuranceFeatures': batch\n",
    "        }\n",
    "    )\n",
    "    # Handle any unprocessed items if needed\n",
    "    unprocessed = response.get('UnprocessedItems', {})\n",
    "    if unprocessed:\n",
    "        print(\"Warning: Some items were not processed. Retry logic needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7c5b2b9-5ec0-469c-a55f-631c7d60e10c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# We ran into more limitations when we tried complex data types as output of a feature function\n",
    "# and then tried to use it for serving\n",
    "# al alternatve solution: using an external database (we use DynamoDB here)\n",
    "\n",
    "# create a DynamoDB table\n",
    "# insert records into dynamo DB & read from dynamoDB\n",
    "\n",
    "# create a pyfunc model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93da8c9a-67c7-41e7-bb64-eedfa5381009",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class InsuranceModelWrapper(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"Wrapper class for machine learning models to be used with MLflow.\n",
    "\n",
    "    This class wraps a machine learning model for predicting house prices.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: object) -> None:\n",
    "        \"\"\"Initialize the HousePriceModelWrapper.\n",
    "\n",
    "        :param model: The underlying machine learning model.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "\n",
    "    def _parse_dynamo_item(self, raw_item):\n",
    "        parsed = {}\n",
    "        for key, val in raw_item.items():\n",
    "            if 'N' in val:\n",
    "                num_val = float(val['N'])\n",
    "                parsed[key] = int(num_val) if num_val.is_integer() else num_val\n",
    "            else:\n",
    "                parsed[key] = val['S']\n",
    "        return parsed\n",
    "\n",
    "    def predict(self, context, model_input: pd.DataFrame | np.ndarray) -> list[int]:\n",
    "        client = boto3.client(\n",
    "            'dynamodb',\n",
    "            aws_access_key_id=os.environ[\"aws_access_key_id\"],\n",
    "            aws_secret_access_key=os.environ[\"aws_secret_access_key\"],\n",
    "            region_name=os.environ[\"region_name\"]\n",
    "        )\n",
    "\n",
    "        parsed = []\n",
    "        for lookup_id in model_input[\"Id\"]:\n",
    "            raw_item = client.get_item(\n",
    "                TableName='InsuranceFeatures',\n",
    "                Key={'Id': {'N': str(int(lookup_id))}}\n",
    "            )[\"Item\"]\n",
    "            parsed_dict = self._parse_dynamo_item(raw_item)\n",
    "            parsed.append(parsed_dict)\n",
    "\n",
    "        lookup_df = pd.DataFrame(parsed)\n",
    "        merged_df = model_input.merge(lookup_df, on=\"Id\", how=\"left\").drop(\"Id\", axis=1)\n",
    "\n",
    "        merged_df[\"age\"] = merged_df[\"age\"].fillna(27)\n",
    "        merged_df[\"bmi\"] = merged_df[\"bmi\"].fillna(30)\n",
    "        merged_df[\"children\"] = merged_df[\"children\"].fillna(2)\n",
    "\n",
    "        predictions = self.model.predict(merged_df)\n",
    "        return [int(x) for x in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4291be8a-66b5-4c34-a613-432a18cefe52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "custom_model = InsuranceModelWrapper(pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a66cbb50-7557-40b5-adfc-6efc1e2cd210",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "features = [f for f in [\"Id\"] + config.num_features + config.cat_features if f not in lookup_features]\n",
    "data = test_set_with_id.select(*features).toPandas()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cd08340-8acb-40fa-be26-5c75cdf23c21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "custom_model.predict(context=None, model_input=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01ceda36-6dc1-4d03-8dd5-31b96cc976c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#log model\n",
    "mlflow.set_experiment(\"/Shared/demo-model-insurance-fe-pyfunc\")\n",
    "with mlflow.start_run(run_name=\"demo-model-insurance-fe-pyfunc\",\n",
    "                      tags={\"git_sha\": \"01234567890george\",\n",
    "                            \"branch\": \"feature3\"},\n",
    "                            description=\"demo run for FE model logging\") as run:\n",
    "    # Log parameters and metrics\n",
    "    run_id = run.info.run_id\n",
    "    mlflow.log_param(\"model_type\", \"LightGBM with preprocessing\")\n",
    "    mlflow.log_params(config.parameters)\n",
    "\n",
    "    # Log the model with feature engineering client. important\n",
    "    signature = infer_signature(model_input=data, model_output=custom_model.predict(context=None, model_input=data))\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"lightgbm-pipeline-model-fe\",\n",
    "        python_model=custom_model,\n",
    "        signature=signature,\n",
    "        )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8bac89b-5c97-4ed5-b7e9-99df310002d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# predict\n",
    "mlflow.models.predict(f\"runs:/{run_id}/lightgbm-pipeline-model-fe\", data[0:1])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7874308232661549,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "week3. feature_engineering_demo.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
