{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6353829-212d-4a58-b11c-c92053c730af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --force-reinstall /dbfs/tmp/insurance-0.0.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f2a62fa-4579-48fa-9c0b-8c25410989b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b0166e2-0208-4eff-bb0e-fccf55f8e2c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# COMMAND ----------\n",
    "import os\n",
    "import time\n",
    "from typing import Dict, List\n",
    "\n",
    "import requests\n",
    "from pyspark.dbutils import DBUtils\n",
    "from pyspark.sql import SparkSession\n",
    "from loguru import logger\n",
    "from insurance.config import ProjectConfig\n",
    "from insurance.serving.fe_model_serving  import  FeatureLookupServing\n",
    "\n",
    "\n",
    "# spark session\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "dbutils = DBUtils(spark)\n",
    "\n",
    "# get environment variables\n",
    "os.environ[\"DBR_TOKEN\"] = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "os.environ[\"DBR_HOST\"] = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "\n",
    "# Load project config\n",
    "config = ProjectConfig.from_yaml(config_path=\"../project_config.yml\")\n",
    "catalog_name = config.catalog_name\n",
    "schema_name = config.schema_name\n",
    "endpoint_name = \"insurance-model-serving-fe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53ee2530-1f85-4bb1-97c0-1c273f71dc86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Feature Lookup Serving Manager\n",
    "feature_model_server = FeatureLookupServing(\n",
    "    model_name=f\"{catalog_name}.{schema_name}.insurance_model_fe_lightgbm\",\n",
    "    endpoint_name=endpoint_name,\n",
    "    feature_table_name=f\"{catalog_name}.{schema_name}.insurance_features\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bac5ecba-d3f5-4077-8614-68932649c557",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the online table for house features\n",
    "feature_model_server.create_online_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f26542b0-175d-4631-a1d1-8e5dbe7bafb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Deploy the model serving endpoint with feature lookup\n",
    "feature_model_server.deploy_or_update_serving_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40679fc0-3753-4724-9638-859c16dea811",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "from pyspark.sql.functions import monotonically_increasing_id, col\n",
    "from pyspark.sql.types import DoubleType, LongType, IntegerType\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "train_set = spark.table(f\"{config.catalog_name}.{config.schema_name}.train_set\").toPandas()\n",
    "\n",
    "\n",
    "\n",
    "# Create a sample request body\n",
    "required_columns = [\"Id\", \"sex\", \"smoker\", \"region\", \"age\", \"bmi\", \"children\"]\n",
    "\n",
    "sampled_records = train_set[required_columns].sample(n=1000, replace=True).to_dict(orient=\"records\")\n",
    "\n",
    "dataframe_records = [[record] for record in sampled_records]\n",
    "\n",
    "logger.info(train_set.dtypes)\n",
    "logger.info(dataframe_records[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "712187a8-0e6a-4c30-b768-27007bc7f2d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "print(json.dumps(dataframe_records[0][0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4466862d-27f4-401f-8d5c-049271cb675c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Call the endpoint with one sample record\n",
    "def call_endpoint(record):\n",
    "    \"\"\"\n",
    "    Calls the model serving endpoint with a given input record.\n",
    "    \"\"\"\n",
    "    serving_endpoint = f\"https://{os.environ['DBR_HOST']}/serving-endpoints/{endpoint_name}/invocations\"\n",
    "\n",
    "    response = requests.post(\n",
    "        serving_endpoint,\n",
    "        headers={\"Authorization\": f\"Bearer {os.environ['DBR_TOKEN']}\"},\n",
    "        json={\"dataframe_records\": record},\n",
    "    )\n",
    "    return response.status_code, response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab8c6a2c-8d1e-4fdd-8eaa-1180d1493f2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "status_code, response_text = call_endpoint(dataframe_records[0])\n",
    "print(f\"Response Status: {status_code}\")\n",
    "print(f\"Response Text: {response_text}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "week4.02_deploy_fe_model_serving",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
